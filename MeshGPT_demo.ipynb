{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tmLTKtUdRmQ",
        "outputId": "48894aeb-e618-42a7-b224-c19ceb32f4ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
            "  Cloning https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to /tmp/pip-req-build-urmhpaon\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git /tmp/pip-req-build-urmhpaon\n",
            "  Resolved https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to commit 5c189f32dbc20cd5882be4ef2a132e2aabcb8df5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (0.16.4)\n",
            "Requirement already satisfied: einops>=0.7 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (6.1.3)\n",
            "Requirement already satisfied: open-clip-torch>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (2.23.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (4.35.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.16.0+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (4.66.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.20.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (3.20.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.9.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2.1.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->classifier-free-guidance-pytorch==0.5.1) (0.2.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.4.1)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (9.4.0)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.10/dist-packages (4.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!yes | pip install git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
        "!yes | pip install -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "!yes | pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7aW7oUHedRmQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")\n",
        "\n",
        "def get_mesh(file_path):\n",
        "    mesh = trimesh.load(file_path, force='mesh')\n",
        "\n",
        "    vertices = mesh.vertices.tolist()\n",
        "    faces = mesh.faces.tolist()\n",
        "    # Center\n",
        "    centered_vertices = vertices - np.mean(vertices, axis=0)\n",
        "\n",
        "    # Calculate the padding and scale factor numerically\n",
        "    padding_fraction = 1 / 128                    # 0.0078125\n",
        "    total_padding = padding_fraction * 2          # 0.015625\n",
        "    scale_factor = 1 - total_padding              # 0.984375\n",
        "\n",
        "    max_abs = np.max(np.abs(centered_vertices))\n",
        "    vertices = centered_vertices / (max_abs / scale_factor)\n",
        "\n",
        "    # Sort by Z, Y,X where Z is vertical\n",
        "    def sort_vertices(vertex):\n",
        "        return vertex[2], vertex[1], vertex[0]\n",
        "\n",
        "    seen = OrderedDict()\n",
        "    for point in vertices:\n",
        "      key = tuple(point)\n",
        "      if key not in seen:\n",
        "        seen[key] = point\n",
        "\n",
        "    unique_vertices =  list(seen.values())\n",
        "    sorted_vertices = sorted(unique_vertices, key=sort_vertices)\n",
        "\n",
        "    vertices_as_tuples = [tuple(v) for v in vertices]\n",
        "    sorted_vertices_as_tuples = [tuple(v) for v in sorted_vertices]\n",
        "\n",
        "    vertex_map = {old_index: new_index for old_index, vertex_tuple in enumerate(vertices_as_tuples) for new_index, sorted_vertex_tuple in enumerate(sorted_vertices_as_tuples) if vertex_tuple == sorted_vertex_tuple}\n",
        "    reindexed_faces = [[vertex_map[face[0]], vertex_map[face[1]], vertex_map[face[2]]] for face in faces]\n",
        "    sorted_faces = [sorted(sub_arr) for sub_arr in reindexed_faces]\n",
        "\n",
        "    return np.array(sorted_vertices), np.array(sorted_faces)\n",
        "\n",
        "\n",
        "def augment_mesh(vertices, scale_factor):\n",
        "    jitter_factor=0.01\n",
        "    possible_values = np.arange(-jitter_factor, jitter_factor , 0.0005)\n",
        "    offsets = np.random.choice(possible_values, size=vertices.shape)\n",
        "    vertices = vertices + offsets\n",
        "\n",
        "    vertices = vertices * scale_factor\n",
        "    return vertices\n",
        "\n",
        "\n",
        "def load_filename(directory, variations):\n",
        "    obj_datas = []\n",
        "    chosen_models_count = {}\n",
        "    possible_values = np.arange(0.75, 1.0 , 0.005)\n",
        "    scale_factors = np.random.choice(possible_values, size=variations)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith((\".glb\")):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vertices, faces = get_mesh(file_path)\n",
        "\n",
        "            faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"cuda\")\n",
        "            face_edges =  derive_face_edges_from_faces(faces)\n",
        "            texts, ext = os.path.splitext(filename)\n",
        "\n",
        "            for scale_factor in scale_factors:\n",
        "                aug_vertices = augment_mesh(vertices.copy(), scale_factor)\n",
        "                obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"cuda\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": texts }\n",
        "                obj_datas.append(obj_data)\n",
        "\n",
        "    print(f\"[create_mesh_dataset] Returning {len(obj_data)} meshes\")\n",
        "    return obj_datas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFoYTLhXhTlT",
        "outputId": "174ce823-0c61-4b24-a565-a34e623bb253"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vthmcnU2dRmS",
        "outputId": "71f7072b-e1f6-4593-ae54-60b8c24ccbff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MeshDataset] Loaded 40 entrys\n",
            "[MeshDataset] Created from 40 entrys\n",
            "dict_keys(['vertices', 'faces', 'face_edges', 'texts'])\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from meshgpt_pytorch import MeshDataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "project_name = \"demo_mesh\"\n",
        "\n",
        "working_dir = f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "working_dir = Path(working_dir)\n",
        "working_dir.mkdir(exist_ok = True, parents = True)\n",
        "dataset_path = working_dir / (project_name + \".npz\")\n",
        "\n",
        "\n",
        "if not os.path.isfile(dataset_path):\n",
        "    data = load_filename(\"/content/drive/MyDrive/demo_mesh\",10)\n",
        "    dataset = MeshDataset(data)\n",
        "    dataset.generate_face_edges()\n",
        "    print(set(item[\"texts\"] for item in dataset.data)  )\n",
        "    dataset.save(dataset_path)\n",
        "\n",
        "dataset = MeshDataset.load(dataset_path)\n",
        "print(dataset.data[0].keys())\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVsRDtzdRmT"
      },
      "source": [
        "### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vPpalWsbdRmU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "folder = f\"{working_dir}/renders\"\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "all_vertices = []\n",
        "all_faces = []\n",
        "vertex_offset = 0\n",
        "translation_distance = 0.5\n",
        "\n",
        "for r, item in enumerate(data):\n",
        "    vertices_copy =  np.copy(item['vertices'].cpu())\n",
        "    vertices_copy += translation_distance * (r / 0.2 - 1)\n",
        "\n",
        "    for vert in vertices_copy:\n",
        "        vertex = vert\n",
        "        all_vertices.append(f\"v {float(vertex[0])}  {float(vertex[1])}  {float(vertex[2])}\\n\")\n",
        "    for face in item['faces']:\n",
        "        all_faces.append(f\"f {face[0]+1+ vertex_offset} {face[1]+ 1+vertex_offset} {face[2]+ 1+vertex_offset}\\n\")\n",
        "    vertex_offset = len(all_vertices)\n",
        "\n",
        "obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "obj_file_path = f'{folder}/3d_models_inspect.obj'\n",
        "\n",
        "with open(obj_file_path, \"w\") as file:\n",
        "    file.write(obj_file_content)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pHZ20udRmU"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xoAxJWecdRmV"
      },
      "outputs": [],
      "source": [
        "autoencoder = MeshAutoencoder().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJewvY1dRmV"
      },
      "source": [
        "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ9qMRVxdRmW",
        "outputId": "be732060-68cb-4a2f-8dea-57843dec8e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n"
          ]
        }
      ],
      "source": [
        "dataset.data = [dict(d) for d in dataset.data] * 10\n",
        "print(len(dataset.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgFGxnXAdRmX"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "9NWrpCCgdRmX",
        "outputId": "56754a3e-983b-40d5-c94d-947089b86ef3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-eca8c5c30c16>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mautoencoder_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeshAutoencoderTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mautoencoder\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mwarmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mgrad_accum_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/drive/MyDrive/demo_mesh/mesh-encoder_{project_name}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mautencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/meshgpt_pytorch/trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mpkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=8,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "autencoder = autoencoder_trainer.model\n",
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9k-OAoBdRmY"
      },
      "source": [
        "**Train to about 0.3 loss if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKTuiTHvdRmY",
        "outputId": "ea372e57-cf37-4f7a-fef7-534122447e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/280: 100%|██████████| 50/50 [00:03<00:00, 14.11it/s, commit_loss=-.319, loss=0.505, recon_loss=0.537]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 average loss: 0.8432329672574997 recon loss: 0.8585: commit_loss -0.1522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/280: 100%|██████████| 50/50 [00:03<00:00, 14.01it/s, commit_loss=-.174, loss=1.17, recon_loss=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 average loss: 0.8369616591930389 recon loss: 0.8540: commit_loss -0.1706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/280: 100%|██████████| 50/50 [00:04<00:00, 10.04it/s, commit_loss=-.194, loss=0.605, recon_loss=0.625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 average loss: 0.8208733612298965 recon loss: 0.8370: commit_loss -0.1611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/280: 100%|██████████| 50/50 [00:04<00:00, 11.66it/s, commit_loss=-.286, loss=0.734, recon_loss=0.763]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 average loss: 0.8308381533622742 recon loss: 0.8489: commit_loss -0.1801          avg loss speed: 0.0028511758645375362 epochs left: 186.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/280: 100%|██████████| 50/50 [00:03<00:00, 13.51it/s, commit_loss=-.0912, loss=0.914, recon_loss=0.923]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 average loss: 0.8474435269832611 recon loss: 0.8664: commit_loss -0.1894          avg loss speed: -0.017885802388191152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/280: 100%|██████████| 50/50 [00:03<00:00, 12.92it/s, commit_loss=-.141, loss=1.08, recon_loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 average loss: 0.8499523031711579 recon loss: 0.8653: commit_loss -0.1537          avg loss speed: -0.016900622646013885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/280: 100%|██████████| 50/50 [00:05<00:00,  9.94it/s, commit_loss=-.0718, loss=0.958, recon_loss=0.966]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 average loss: 0.8294972753524781 recon loss: 0.8465: commit_loss -0.1702          avg loss speed: 0.013247385819752933 epochs left: 39.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/280: 100%|██████████| 50/50 [00:04<00:00, 12.40it/s, commit_loss=-.158, loss=0.868, recon_loss=0.884]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 average loss: 0.8366289049386978 recon loss: 0.8536: commit_loss -0.1693          avg loss speed: 0.005668796896934447 epochs left: 94.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/280: 100%|██████████| 50/50 [00:03<00:00, 14.17it/s, commit_loss=-.21, loss=1.36, recon_loss=1.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 average loss: 0.8474504554271698 recon loss: 0.8649: commit_loss -0.1743          avg loss speed: -0.00875762760639176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/280: 100%|██████████| 50/50 [00:04<00:00, 11.18it/s, commit_loss=-.273, loss=1.02, recon_loss=1.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 average loss: 0.8503154933452606 recon loss: 0.8678: commit_loss -0.1749          avg loss speed: -0.012456614772478614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/280: 100%|██████████| 50/50 [00:03<00:00, 13.55it/s, commit_loss=-.188, loss=1.19, recon_loss=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 average loss: 0.818656479716301 recon loss: 0.8369: commit_loss -0.1824          avg loss speed: 0.026141804854075135 epochs left: 19.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/280: 100%|██████████| 50/50 [00:03<00:00, 14.00it/s, commit_loss=-.199, loss=0.94, recon_loss=0.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 average loss: 0.7962182116508484 recon loss: 0.8140: commit_loss -0.1776          avg loss speed: 0.04258926451206202 epochs left: 11.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/280: 100%|██████████| 50/50 [00:03<00:00, 13.88it/s, commit_loss=-.216, loss=0.907, recon_loss=0.929]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 average loss: 0.8037458086013793 recon loss: 0.8226: commit_loss -0.1881          avg loss speed: 0.017984252969423964 epochs left: 28.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/280: 100%|██████████| 50/50 [00:05<00:00,  9.11it/s, commit_loss=-.264, loss=0.669, recon_loss=0.695]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 average loss: 0.82264264523983 recon loss: 0.8413: commit_loss -0.1868          avg loss speed: -0.016435811916987175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/280: 100%|██████████| 50/50 [00:04<00:00, 10.30it/s, commit_loss=-.34, loss=0.498, recon_loss=0.532]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 average loss: 0.8214196532964706 recon loss: 0.8391: commit_loss -0.1767          avg loss speed: -0.013884098132451284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/280: 100%|██████████| 50/50 [00:04<00:00, 11.73it/s, commit_loss=-.164, loss=0.492, recon_loss=0.509]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 average loss: 0.7915117239952087 recon loss: 0.8106: commit_loss -0.1911          avg loss speed: 0.024424311717351377 epochs left: 20.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/280: 100%|██████████| 50/50 [00:07<00:00,  6.71it/s, commit_loss=-.186, loss=0.834, recon_loss=0.853]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 average loss: 0.82513925075531 recon loss: 0.8450: commit_loss -0.1982          avg loss speed: -0.013281243244807062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/280: 100%|██████████| 50/50 [00:04<00:00, 11.55it/s, commit_loss=-.161, loss=0.671, recon_loss=0.687]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 average loss: 0.8113855820894241 recon loss: 0.8306: commit_loss -0.1921          avg loss speed: 0.0013046272595722552 epochs left: 391.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/280: 100%|██████████| 50/50 [00:03<00:00, 13.32it/s, commit_loss=-.369, loss=0.571, recon_loss=0.608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 average loss: 0.8258328437805176 recon loss: 0.8448: commit_loss -0.1893          avg loss speed: -0.016487324833869832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/280: 100%|██████████| 50/50 [00:04<00:00, 11.12it/s, commit_loss=-.209, loss=0.788, recon_loss=0.809]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 average loss: 0.8238621890544892 recon loss: 0.8418: commit_loss -0.1796          avg loss speed: -0.003076296846071913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/280: 100%|██████████| 50/50 [00:03<00:00, 14.02it/s, commit_loss=-.194, loss=0.848, recon_loss=0.868]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 average loss: 0.8278667843341827 recon loss: 0.8466: commit_loss -0.1874          avg loss speed: -0.007506579359372378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/280: 100%|██████████| 50/50 [00:03<00:00, 14.37it/s, commit_loss=-.23, loss=0.908, recon_loss=0.931]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 average loss: 0.9118725955486298 recon loss: 0.9308: commit_loss -0.1895          avg loss speed: -0.08601865649223328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/280: 100%|██████████| 50/50 [00:03<00:00, 12.59it/s, commit_loss=-.139, loss=0.81, recon_loss=0.824]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 average loss: 0.8489308542013169 recon loss: 0.8639: commit_loss -0.1495          avg loss speed: 0.0056030021111169726 epochs left: 97.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/280: 100%|██████████| 50/50 [00:04<00:00, 12.05it/s, commit_loss=-.14, loss=0.81, recon_loss=0.824]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 average loss: 0.820583564043045 recon loss: 0.8386: commit_loss -0.1805          avg loss speed: 0.042306513984998184 epochs left: 12.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/280: 100%|██████████| 50/50 [00:03<00:00, 14.44it/s, commit_loss=-.103, loss=0.871, recon_loss=0.881]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 average loss: 0.7937628495693206 recon loss: 0.8117: commit_loss -0.1797          avg loss speed: 0.06669948836167672 epochs left: 7.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/280: 100%|██████████| 50/50 [00:03<00:00, 14.03it/s, commit_loss=-.12, loss=0.802, recon_loss=0.814]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 average loss: 0.8060168963670731 recon loss: 0.8235: commit_loss -0.1745          avg loss speed: 0.015075526237487757 epochs left: 33.57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/280: 100%|██████████| 50/50 [00:04<00:00, 12.22it/s, commit_loss=-.161, loss=0.966, recon_loss=0.982]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 average loss: 0.7903568947315216 recon loss: 0.8086: commit_loss -0.1826          avg loss speed: 0.016430875261624567 epochs left: 29.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/280: 100%|██████████| 50/50 [00:04<00:00, 12.49it/s, commit_loss=-.333, loss=0.41, recon_loss=0.443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 average loss: 0.7615941172838211 recon loss: 0.7793: commit_loss -0.1768          avg loss speed: 0.03511809627215068 epochs left: 13.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/280: 100%|██████████| 50/50 [00:03<00:00, 14.16it/s, commit_loss=-.251, loss=0.716, recon_loss=0.741]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 average loss: 0.677488683462143 recon loss: 0.6975: commit_loss -0.2005          avg loss speed: 0.10850061933199562 epochs left: 3.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/280: 100%|██████████| 50/50 [00:03<00:00, 14.31it/s, commit_loss=-.13, loss=0.589, recon_loss=0.602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 average loss: 0.6526379525661469 recon loss: 0.6702: commit_loss -0.1757          avg loss speed: 0.09050861259301501 epochs left: 3.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/280: 100%|██████████| 50/50 [00:04<00:00, 11.67it/s, commit_loss=-.23, loss=0.956, recon_loss=0.979]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 average loss: 0.5932538884878159 recon loss: 0.6113: commit_loss -0.1809          avg loss speed: 0.10398636261622107 epochs left: 2.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/280: 100%|██████████| 50/50 [00:03<00:00, 12.91it/s, commit_loss=-.135, loss=0.572, recon_loss=0.585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 average loss: 0.5874183249473571 recon loss: 0.6061: commit_loss -0.1863          avg loss speed: 0.053708516558011565 epochs left: 5.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/280: 100%|██████████| 50/50 [00:03<00:00, 12.52it/s, commit_loss=-.0946, loss=0.597, recon_loss=0.606]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 average loss: 0.5816872483491897 recon loss: 0.6007: commit_loss -0.1900          avg loss speed: 0.02941614031791684 epochs left: 9.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/280: 100%|██████████| 50/50 [00:03<00:00, 14.06it/s, commit_loss=-.207, loss=0.604, recon_loss=0.625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 average loss: 0.5519315600395203 recon loss: 0.5715: commit_loss -0.1954          avg loss speed: 0.035521593888600655 epochs left: 7.09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/280: 100%|██████████| 50/50 [00:04<00:00, 10.96it/s, commit_loss=-.159, loss=0.656, recon_loss=0.672]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 average loss: 0.5112252014875412 recon loss: 0.5296: commit_loss -0.1842          avg loss speed: 0.06245384295781442 epochs left: 3.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/280: 100%|██████████| 50/50 [00:03<00:00, 14.10it/s, commit_loss=-.238, loss=0.553, recon_loss=0.577]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 average loss: 0.48497660756111144 recon loss: 0.5038: commit_loss -0.1884          avg loss speed: 0.06330472906430568 epochs left: 2.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/280: 100%|██████████| 50/50 [00:03<00:00, 13.97it/s, commit_loss=-.209, loss=0.404, recon_loss=0.425]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 average loss: 0.47614225149154665 recon loss: 0.4955: commit_loss -0.1935          avg loss speed: 0.03990220487117768 epochs left: 4.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/280: 100%|██████████| 50/50 [00:03<00:00, 12.94it/s, commit_loss=-.148, loss=0.568, recon_loss=0.583]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 average loss: 0.446226099729538 recon loss: 0.4653: commit_loss -0.1905          avg loss speed: 0.04455525378386177 epochs left: 3.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/280: 100%|██████████| 50/50 [00:04<00:00, 11.44it/s, commit_loss=-.247, loss=0.365, recon_loss=0.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 average loss: 0.4129255497455597 recon loss: 0.4316: commit_loss -0.1866          avg loss speed: 0.05618943651517233 epochs left: 2.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/280: 100%|██████████| 50/50 [00:03<00:00, 14.19it/s, commit_loss=-.36, loss=0.303, recon_loss=0.339]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 average loss: 0.4071366423368454 recon loss: 0.4271: commit_loss -0.1998          avg loss speed: 0.037961324652036055 epochs left: 2.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/280: 100%|██████████| 50/50 [00:03<00:00, 13.85it/s, commit_loss=-.122, loss=0.339, recon_loss=0.352]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 average loss: 0.38592670261859896 recon loss: 0.4061: commit_loss -0.2020          avg loss speed: 0.03616939465204877 epochs left: 2.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/280: 100%|██████████| 50/50 [00:04<00:00, 12.25it/s, commit_loss=-.198, loss=0.421, recon_loss=0.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 average loss: 0.36275006413459776 recon loss: 0.3824: commit_loss -0.1963          avg loss speed: 0.03924623409907024 epochs left: 1.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/280: 100%|██████████| 50/50 [00:04<00:00, 11.98it/s, commit_loss=-.274, loss=0.307, recon_loss=0.335]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 average loss: 0.3377884659171104 recon loss: 0.3586: commit_loss -0.2084          avg loss speed: 0.04748267044623694 epochs left: 0.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/280: 100%|██████████| 50/50 [00:03<00:00, 14.32it/s, commit_loss=-.181, loss=0.416, recon_loss=0.434]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 average loss: 0.3593804454803467 recon loss: 0.3813: commit_loss -0.2196          avg loss speed: 0.002774632076422312 epochs left: 21.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/280: 100%|██████████| 50/50 [00:03<00:00, 13.94it/s, commit_loss=-.0459, loss=0.442, recon_loss=0.446]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 average loss: 0.5032330232858658 recon loss: 0.5228: commit_loss -0.1955          avg loss speed: -0.14992669810851422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/280: 100%|██████████| 50/50 [00:04<00:00, 11.56it/s, commit_loss=-.319, loss=0.331, recon_loss=0.363]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 average loss: 0.38060089349746706 recon loss: 0.3990: commit_loss -0.1845          avg loss speed: 0.019533084730307293 epochs left: 4.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/280: 100%|██████████| 50/50 [00:03<00:00, 13.04it/s, commit_loss=-.199, loss=0.331, recon_loss=0.351]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 average loss: 0.3384891849756241 recon loss: 0.3576: commit_loss -0.1914          avg loss speed: 0.07591560244560241 epochs left: 0.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/280: 100%|██████████| 50/50 [00:03<00:00, 13.83it/s, commit_loss=-.211, loss=0.31, recon_loss=0.331]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 average loss: 0.3199825304746628 recon loss: 0.3384: commit_loss -0.1838          avg loss speed: 0.08745850344498957 epochs left: 0.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/280: 100%|██████████| 50/50 [00:03<00:00, 14.19it/s, commit_loss=-.0946, loss=0.306, recon_loss=0.316]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 average loss: 0.307504768371582 recon loss: 0.3282: commit_loss -0.2074          avg loss speed: 0.03885276794433595 epochs left: 0.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/280: 100%|██████████| 50/50 [00:04<00:00, 11.02it/s, commit_loss=-.124, loss=0.307, recon_loss=0.319]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 average loss: 0.30487499475479124 recon loss: 0.3261: commit_loss -0.2125          avg loss speed: 0.01711716651916506 epochs left: 0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51/280: 100%|██████████| 50/50 [00:03<00:00, 13.73it/s, commit_loss=-.221, loss=0.298, recon_loss=0.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51 average loss: 0.2999150338768959 recon loss: 0.3219: commit_loss -0.2199          avg loss speed: 0.010872397323449468 epochs left: 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 52/280:  44%|████▍     | 22/50 [00:02<00:02, 11.42it/s, commit_loss=-.13, loss=0.298, recon_loss=0.311] "
          ]
        }
      ],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
        "                                             batch_size=8,\n",
        "                                             grad_accum_every=2,\n",
        "                                             learning_rate = 4e-3)\n",
        "loss = autoencoder_trainer.train(280,stop_at_loss = 0.28, diplay_graph= True)\n",
        "autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLCezPnNdRmZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Decoder total parameters: {total_params}\")\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8sv-udRmZ"
      },
      "source": [
        "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
        "\n",
        "**If you don't;** <br>\n",
        "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
        "<br>\n",
        "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
        "\n",
        "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s--ya8W0dRmZ"
      },
      "outputs": [],
      "source": [
        "labels = set(item[\"texts\"] for item in dataset.data)\n",
        "print(labels)\n",
        "dataset.embed_texts(transformer)\n",
        "dataset.generate_codes(autoencoder)\n",
        "print(dataset.data[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFa_p1G-dRma"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9hV_xUQdRma"
      },
      "outputs": [],
      "source": [
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "transformer = trainer.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXZ0qgV3dRma"
      },
      "source": [
        "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhcQE6XIdRma"
      },
      "outputs": [],
      "source": [
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
        "                                 learning_rate = 1e-3, batch_size=2)\n",
        "loss = trainer.train(100, stop_at_loss = 0.009)\n",
        "\n",
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
        "                                 learning_rate = 5e-4, batch_size=2)\n",
        "loss = trainer.train(200, stop_at_loss = 0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRlzLzYxdRma"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer.save(f'{working_dir}/mesh-transformer_{project_name}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIn8JVsNdRma"
      },
      "source": [
        "## Generate and view mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX4RM2-ddRma"
      },
      "outputs": [],
      "source": [
        "def combind_mesh(path, mesh):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for r, faces_coordinates in enumerate(mesh):\n",
        "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "\n",
        "        for vertex in numpy_data:\n",
        "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "        for i in range(1, len(numpy_data), 3):\n",
        "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "        vertex_offset += len(numpy_data)\n",
        "\n",
        "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "def combind_mesh_with_rows(path, meshes):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for row, mesh in enumerate(meshes):\n",
        "        for r, faces_coordinates in enumerate(mesh):\n",
        "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)\n",
        "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)\n",
        "\n",
        "            for vertex in numpy_data:\n",
        "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "            for i in range(1, len(numpy_data), 3):\n",
        "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "            vertex_offset += len(numpy_data)\n",
        "\n",
        "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "\n",
        "def write_mesh_output(path, coords):\n",
        "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "    obj_file_content = \"\"\n",
        "\n",
        "    for vertex in numpy_data:\n",
        "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
        "\n",
        "    for i in range(1, len(numpy_data), 3):\n",
        "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
        "\n",
        "    with open(path, \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdd-0bMJdRma"
      },
      "source": [
        "**Using only text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzAkhWM7dRmb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "text_coords = []\n",
        "for text in labels:\n",
        "    print(f\"Generating {text}\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0)\n",
        "    text_coords.append(faces_coordinates)\n",
        "\n",
        "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZP_cLvdRmb"
      },
      "source": [
        "**Text + prompt of tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8GVxRnrdRmb"
      },
      "source": [
        "Grab fresh copy of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ni22DzdRmb"
      },
      "outputs": [],
      "source": [
        "dataset = MeshDataset.load(dataset_path)\n",
        "dataset.generate_codes(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJxZBNUdRmb"
      },
      "source": [
        "**Prompt with 10% of codes/tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NTGFLFdRmb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "token_length_procent = 0.10\n",
        "codes = []\n",
        "texts = []\n",
        "for label in labels:\n",
        "    for item in dataset.data:\n",
        "        if item['texts'] == label:\n",
        "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "            texts.append(item['texts'])\n",
        "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "            break\n",
        "\n",
        "folder = working_dir / f'renders/text+codes'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "coords = []\n",
        "\n",
        "\n",
        "\n",
        "for text, prompt in zip(texts, codes):\n",
        "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "    coords.append(faces_coordinates)\n",
        "\n",
        "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "    write_mesh_output(obj_file_path, faces_coordinates)\n",
        "\n",
        "    print(obj_file_path)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
        "\n",
        "if text_coords is not None:\n",
        "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBZsZJtdRmb"
      },
      "source": [
        "**Prompt with 0% to 80% of tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUYkxcwdRmc"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / f'renders/text+codes_rows'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "mesh_rows = []\n",
        "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
        "    codes = []\n",
        "    texts = []\n",
        "    for label in labels:\n",
        "        for item in dataset.data:\n",
        "            if item['texts'] == label:\n",
        "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "                texts.append(item['texts'])\n",
        "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "                break\n",
        "\n",
        "    coords = []\n",
        "    for text, prompt in zip(texts, codes):\n",
        "\n",
        "        print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "        faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "        coords.append(faces_coordinates)\n",
        "\n",
        "        obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "        write_mesh_output(obj_file_path, coords)\n",
        "        print(obj_file_path)\n",
        "\n",
        "\n",
        "    mesh_rows.append(coords)\n",
        "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
        "\n",
        "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCt_LvARdRmc"
      },
      "source": [
        "**Just some testing for text embedding similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yejz0vYdRmc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "texts = list(labels)\n",
        "vectors = [transformer.conditioner.text_models[0].embed_text([text], return_text_encodings = False).cpu().flatten() for text in texts]\n",
        "\n",
        "max_label_length = max(len(text) for text in texts)\n",
        "\n",
        "# Print the table header\n",
        "print(f\"{'Text':<{max_label_length}} |\", end=\" \")\n",
        "for text in texts:\n",
        "    print(f\"{text:<{max_label_length}} |\", end=\" \")\n",
        "print()\n",
        "\n",
        "# Print the similarity matrix as a table with fixed-length columns\n",
        "for i in range(len(texts)):\n",
        "    print(f\"{texts[i]:<{max_label_length}} |\", end=\" \")\n",
        "    for j in range(len(texts)):\n",
        "        # Encode the texts and calculate cosine similarity manually\n",
        "        vector_i = vectors[i]\n",
        "        vector_j = vectors[j]\n",
        "\n",
        "        dot_product = torch.sum(vector_i * vector_j)\n",
        "        norm_vector1 = torch.norm(vector_i)\n",
        "        norm_vector2 = torch.norm(vector_j)\n",
        "        similarity_score = dot_product / (norm_vector1 * norm_vector2)\n",
        "\n",
        "        # Print with fixed-length columns\n",
        "        print(f\"{similarity_score.item():<{max_label_length}.4f} |\", end=\" \")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}