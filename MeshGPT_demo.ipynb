{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V-Sekai-fire/meshgpt-dataset-01/blob/main/MeshGPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_train_autoencoder = True\n",
        "is_train_mesh_transformer = True\n",
        "is_train_autoencoder_iteration = True\n",
        "is_clear_dataset_npz = True"
      ],
      "metadata": {
        "id": "NM_rRocQAcZ_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aFoYTLhXhTlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e0a935-2ea5-4bbe-f4a4-05a238a0ddcc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9tmLTKtUdRmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd423305-565c-4cb2-b5c9-6e5735d5fb5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: typing_extensions 4.9.0\n",
            "Uninstalling typing_extensions-4.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/typing_extensions-4.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "Proceed (Y/n)?   Successfully uninstalled typing_extensions-4.9.0\n",
            "Collecting git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
            "  Cloning https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to /tmp/pip-req-build-mhhikok4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git /tmp/pip-req-build-mhhikok4\n",
            "  Resolved https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to commit 5c189f32dbc20cd5882be4ef2a132e2aabcb8df5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (0.16.4)\n",
            "Requirement already satisfied: einops>=0.7 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (6.1.3)\n",
            "Requirement already satisfied: open-clip-torch>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (2.23.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (4.35.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.16.0+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (4.66.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.20.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (3.20.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.9.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.13.1)\n",
            "Collecting typing-extensions (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1)\n",
            "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2.1.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->classifier-free-guidance-pytorch==0.5.1) (0.2.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.4.1)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (9.4.0)\n",
            "Installing collected packages: typing-extensions\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.9.0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.10/dist-packages (4.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!yes | pip uninstall typing-extensions\n",
        "!yes | pip install git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
        "!yes | pip install -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "!yes | pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")"
      ],
      "metadata": {
        "id": "5ztZ1JUl8zOZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7aW7oUHedRmQ"
      },
      "outputs": [],
      "source": [
        "import trimesh\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "max_faces = 5000\n",
        "\n",
        "def get_mesh(file_path):\n",
        "    mesh = trimesh.load(file_path, force='mesh')\n",
        "\n",
        "    # Center and scale vertices\n",
        "    center = np.mean(mesh.vertices, axis=0)\n",
        "    vertices = mesh.vertices - center\n",
        "    max_abs = np.max(np.abs(vertices))\n",
        "    scale_factor = (1 / 128) / max_abs\n",
        "    vertices *= scale_factor\n",
        "\n",
        "    # Quantize vertices\n",
        "    vertices = np.around(vertices).astype(np.float32)\n",
        "\n",
        "    # Sort vertices by Z, Y, X\n",
        "    sorted_indices = np.lexsort(vertices.T[::-1])\n",
        "    vertices = vertices[sorted_indices]\n",
        "\n",
        "    # Map old indices to new, sorted indices\n",
        "    vertex_map = np.empty(len(sorted_indices), dtype=int)\n",
        "    vertex_map[sorted_indices] = np.arange(len(sorted_indices))\n",
        "\n",
        "    # Reindex faces\n",
        "    reindexed_faces = vertex_map[mesh.faces]\n",
        "    sorted_faces = np.sort(reindexed_faces, axis=1)\n",
        "\n",
        "    return vertices, sorted_faces\n",
        "\n",
        "def augment_mesh(vertices, jitter_strength=0.01):\n",
        "    jitter_amount = np.random.uniform(-jitter_strength, jitter_strength, size=vertices.shape)\n",
        "    vertices += jitter_amount\n",
        "    return vertices\n",
        "\n",
        "def snake_to_sentence_case(snake_str):\n",
        "    components = snake_str.split(\"_\")\n",
        "    return \" \".join(word.capitalize() for word in components)\n",
        "\n",
        "def load_filename(directory, variations):\n",
        "    obj_datas = []\n",
        "\n",
        "    # Get random scale factors within a range\n",
        "    scale_factors = np.random.uniform(0.75, 1.0, size=variations)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".glb\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vertices, faces = get_mesh(file_path)\n",
        "\n",
        "            if len(faces) > max_faces:\n",
        "                print(f\"Mesh {filename} has {len(faces)} faces which is more than the allowed {max_faces} faces. Rejecting.\")\n",
        "                continue\n",
        "\n",
        "            faces_tensor = torch.tensor(faces, dtype=torch.long).to(\"cuda\")\n",
        "            face_edges = derive_face_edges_from_faces(faces_tensor)\n",
        "\n",
        "            texts, _ = os.path.splitext(filename)\n",
        "            texts = snake_to_sentence_case(texts)\n",
        "            # Run video llava on the image. \"Describe the focus of the photo as a json dictionary.\"\n",
        "            for scale_factor in scale_factors:\n",
        "                aug_vertices = augment_mesh(vertices.copy()) * scale_factor\n",
        "                aug_vertices_tensor = torch.tensor(aug_vertices, dtype=torch.float)\n",
        "\n",
        "                obj_data = {\n",
        "                    \"vertices\": aug_vertices_tensor.to(\"cuda\"),\n",
        "                    \"faces\": faces_tensor,\n",
        "                    \"face_edges\": face_edges,\n",
        "                    \"texts\": texts\n",
        "                }\n",
        "                obj_datas.append(obj_data)\n",
        "\n",
        "    print(f\"[create_mesh_dataset] Returning {len(obj_datas)} meshes\")\n",
        "    return obj_datas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vthmcnU2dRmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcaa12a5-795f-44ef-dbae-8e57bb782340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesh sk_masculine_teen.glb has 13408 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh sk_masculine_adult_ik.glb has 13410 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh sk_baby_01.glb has 13408 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh sk_masculine_adult.glb has 13410 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh sk_feminine_teen.glb has 13358 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_vehicle_bicycle_bmx.glb has 39972 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_table_coffee_variation.glb has 8140 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_chair_office_01.glb has 25192 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_sgcontroller.glb has 8848 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh sk_feminine_adult.glb has 13358 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh sk_child_01.glb has 13408 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_desk.glb has 7240 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_form_torso_dress_feminine.glb has 21754 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh sk_dog_01.glb has 8418 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_skate_park.glb has 8598 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "Mesh s_chair_office.glb has 6894 faces which is more than the allowed 5000 faces. Rejecting.\n",
            "[create_mesh_dataset] Returning 47 meshes\n",
            "[MeshDataset] Created from 47 entrys\n",
            "[MeshDataset] Generated face_edges for 0/47 entrys\n",
            "[MeshDataset] Saved 47 entrys at /content/drive/MyDrive/demo_mesh/demo_mesh.npz\n",
            "dict_keys(['vertices', 'faces', 'face_edges', 'texts'])\n",
            "{'S Door Double', 'S Primitive Wedge', 'S Table Sit Square', 'S Chair Modern', 'S Table Counter', 'S Primitive Cylinder Hollow', 'S Table Office', 'S Door Single', 'S Door Double Frame', 'S Table Nightstand', 'S Table Bar Circle', 'S Chair Stool', 'S Door Single Frame', 'S Mask', 'S Bed King', 'Sk Horse 01', 'S Table Coffee', 'S Primitive Pyramid', 'S Bone', 'S Cabinet Dresser 03', 'S Stairs Single-6', 'S Table Bedside', 'Mire Clothing', 'S Primitive Cylinder', 'S Bed Full', 'S Tree Bushy', 'S Chair Sofa Wide', 'S Cabinet Bookshelf', 'S Ziggurat', 'S Table Bar Rectangle', 'S Primitive Sphere', 'S Hmd', 'S Box', 'S Phone', 'S Chair Stool Mini', 'S Table Bar', 'S Tree No Leaves', 'Sk Cat 01', 'S Table Sit Rectangle', 'S Chair Bar', 'S Bed Twin', 'S Chair Sofa', 'Sk Snake 01', 'S Chair Box', 'S Cabinet Dresser 05', 'S Gui', 'S Table Sit Circle'}\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from meshgpt_pytorch import MeshDataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "project_name = \"demo_mesh\"\n",
        "\n",
        "working_dir = f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "working_dir = Path(working_dir)\n",
        "working_dir.mkdir(exist_ok = True, parents = True)\n",
        "dataset_path = working_dir / (project_name + \".npz\")\n",
        "\n",
        "\n",
        "if is_clear_dataset_npz:\n",
        "    data = load_filename(working_dir, 1)\n",
        "    dataset = MeshDataset(data)\n",
        "    dataset.generate_face_edges()\n",
        "    dataset.save(dataset_path)\n",
        "else:\n",
        "    dataset = MeshDataset.load(dataset_path)\n",
        "\n",
        "print(dataset.data[0].keys())\n",
        "print(set(item[\"texts\"] for item in dataset.data)  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVsRDtzdRmT"
      },
      "source": [
        "### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vPpalWsbdRmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00113842-ca18-45c8-87d2-06885663461a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 complete. Processed 403 vertices and 480 faces.\n",
            "Iteration 1 complete. Processed 403 vertices and 480 faces.\n",
            "Iteration 2 complete. Processed 336 vertices and 168 faces.\n",
            "Iteration 3 complete. Processed 264 vertices and 335 faces.\n",
            "Iteration 4 complete. Processed 168 vertices and 144 faces.\n",
            "Iteration 5 complete. Processed 168 vertices and 144 faces.\n",
            "Iteration 6 complete. Processed 205 vertices and 320 faces.\n",
            "Iteration 7 complete. Processed 216 vertices and 108 faces.\n",
            "Iteration 8 complete. Processed 128 vertices and 256 faces.\n",
            "Iteration 9 complete. Processed 96 vertices and 48 faces.\n",
            "Iteration 10 complete. Processed 96 vertices and 48 faces.\n",
            "Iteration 11 complete. Processed 62 vertices and 48 faces.\n",
            "Iteration 12 complete. Processed 130 vertices and 128 faces.\n",
            "Iteration 13 complete. Processed 56 vertices and 24 faces.\n",
            "Iteration 14 complete. Processed 38 vertices and 44 faces.\n",
            "Iteration 15 complete. Processed 38 vertices and 44 faces.\n",
            "Iteration 16 complete. Processed 24 vertices and 12 faces.\n",
            "Iteration 17 complete. Processed 18 vertices and 8 faces.\n",
            "Iteration 18 complete. Processed 16 vertices and 6 faces.\n",
            "Iteration 19 complete. Processed 1848 vertices and 912 faces.\n",
            "Iteration 20 complete. Processed 1280 vertices and 1306 faces.\n",
            "Iteration 21 complete. Processed 1917 vertices and 2320 faces.\n",
            "Iteration 22 complete. Processed 280 vertices and 556 faces.\n",
            "Iteration 23 complete. Processed 1486 vertices and 1376 faces.\n",
            "Iteration 24 complete. Processed 451 vertices and 626 faces.\n",
            "Iteration 25 complete. Processed 1089 vertices and 2090 faces.\n",
            "Iteration 26 complete. Processed 637 vertices and 564 faces.\n",
            "Iteration 27 complete. Processed 848 vertices and 564 faces.\n",
            "Iteration 28 complete. Processed 1142 vertices and 1464 faces.\n",
            "Iteration 29 complete. Processed 2243 vertices and 4304 faces.\n",
            "Iteration 30 complete. Processed 40 vertices and 20 faces.\n",
            "Iteration 31 complete. Processed 432 vertices and 216 faces.\n",
            "Iteration 32 complete. Processed 552 vertices and 256 faces.\n",
            "Iteration 33 complete. Processed 2179 vertices and 3884 faces.\n",
            "Iteration 34 complete. Processed 385 vertices and 440 faces.\n",
            "Iteration 35 complete. Processed 2388 vertices and 2184 faces.\n",
            "Iteration 36 complete. Processed 796 vertices and 910 faces.\n",
            "Iteration 37 complete. Processed 1197 vertices and 1486 faces.\n",
            "Iteration 38 complete. Processed 2074 vertices and 3638 faces.\n",
            "Iteration 39 complete. Processed 744 vertices and 344 faces.\n",
            "Iteration 40 complete. Processed 904 vertices and 1176 faces.\n",
            "Iteration 41 complete. Processed 3645 vertices and 4447 faces.\n",
            "Iteration 42 complete. Processed 702 vertices and 784 faces.\n",
            "Iteration 43 complete. Processed 1486 vertices and 1376 faces.\n",
            "Iteration 44 complete. Processed 3112 vertices and 1556 faces.\n",
            "Iteration 45 complete. Processed 690 vertices and 512 faces.\n",
            "Iteration 46 complete. Processed 3648 vertices and 4424 faces.\n",
            "Total number of processed meshes: 47\n"
          ]
        }
      ],
      "source": [
        "seen_texts = []  # Keep track of seen texts to avoid processing duplicates\n",
        "mesh_list = []  # List to store individual meshes\n",
        "translation_distance = 1.0  # Distance to translate vertices, set to 1 unit as required\n",
        "\n",
        "# Iterate over each item in the dataset\n",
        "for r, item in enumerate(dataset.data):\n",
        "    texts = item['texts']\n",
        "\n",
        "    # Skip this iteration if there are no texts or if we've already seen these texts\n",
        "    if len(texts) == 0 or texts in seen_texts:\n",
        "        continue\n",
        "\n",
        "    # Add these texts to the list of seen texts\n",
        "    seen_texts.append(texts)\n",
        "\n",
        "    # Get vertices and faces\n",
        "    vertices = np.array(item['vertices'].cpu())\n",
        "    faces = np.array(item['faces'].cpu())\n",
        "\n",
        "    # Translate the vertices copy\n",
        "    translation_vector = np.array([len(seen_texts) * translation_distance, 0, 0])  # Translation along the x-axis\n",
        "    vertices[:, :3] += translation_vector  # Apply translation only to x, y, z\n",
        "\n",
        "    # Create a new mesh object with the translated vertices and original faces\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "\n",
        "    # Append the new mesh to our list of meshes\n",
        "    mesh_list.append(mesh)\n",
        "\n",
        "    print(f\"Iteration {r} complete. Processed {len(vertices)} vertices and {len(faces)} faces.\")\n",
        "\n",
        "# After iterating over all items, print the number of processed meshes\n",
        "print(f\"Total number of processed meshes: {len(mesh_list)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pHZ20udRmU"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xoAxJWecdRmV"
      },
      "outputs": [],
      "source": [
        "autoencoder = MeshAutoencoder().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJewvY1dRmV"
      },
      "source": [
        "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JJ9qMRVxdRmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e2ad9c-2ad3-458f-ff8f-cc0c439d4897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "423\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(dataset.data)\n",
        "\n",
        "initial_length = len(dataset.data)\n",
        "if initial_length > 0:\n",
        "    replication_factor = max(1, (400 - 1) // initial_length + 1)\n",
        "    dataset.data *= replication_factor\n",
        "    dataset.data = dataset.data[:2000]\n",
        "\n",
        "print(len(dataset.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9k-OAoBdRmY"
      },
      "source": [
        "**Train to about 0.3 loss if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NKTuiTHvdRmY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "610435f4-5160-46fd-c50e-1f4e7cf521cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/380: 100%|██████████| 52/52 [00:14<00:00,  3.47it/s, commit_loss=-.425, loss=0.22, recon_loss=0.262]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 average loss: 0.22340341972617003 recon loss: 0.2564: commit_loss -0.3302\n",
            "Stopping training at epoch 0 with average loss 0.22340341972617003\n",
            "Training complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
        "                                            batch_size=8,\n",
        "                                            grad_accum_every=2,\n",
        "                                            learning_rate = 4e-3)\n",
        "if is_train_autoencoder:\n",
        "  if is_train_autoencoder_iteration:\n",
        "    autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  loss = autoencoder_trainer.train(380,stop_at_loss = 0.28, diplay_graph= True)\n",
        "  autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "else:\n",
        "  autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  autencoder = autoencoder_trainer.model\n",
        "  for param in autoencoder.parameters():\n",
        "      param.requires_grad = True\n",
        "  import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gLCezPnNdRmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3cc96d-bf01-497e-97b2-4badfa50b78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest face count: 4447\n",
            "Max token sequence: 26682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder total parameters: 94.4M\n",
            "Total parameters: 154.3M\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Decoder total parameters: {total_params}\")\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8sv-udRmZ"
      },
      "source": [
        "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
        "\n",
        "**If you don't;** <br>\n",
        "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
        "<br>\n",
        "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
        "\n",
        "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s--ya8W0dRmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cb77c5-3450-4c97-b80d-cd9be3ae9773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'S Door Double', 'S Primitive Wedge', 'S Table Sit Square', 'S Chair Modern', 'S Table Counter', 'S Table Office', 'S Primitive Cylinder Hollow', 'S Door Single', 'S Table Nightstand', 'S Table Bar Circle', 'S Chair Stool', 'S Door Double Frame', 'S Door Single Frame', 'S Mask', 'S Bed King', 'Sk Horse 01', 'S Table Coffee', 'S Primitive Pyramid', 'S Bone', 'S Cabinet Dresser 03', 'S Stairs Single-6', 'S Table Bedside', 'Mire Clothing', 'S Primitive Cylinder', 'S Bed Full', 'S Chair Sofa Wide', 'S Cabinet Bookshelf', 'S Tree Bushy', 'S Ziggurat', 'S Table Bar', 'S Chair Stool Mini', 'S Hmd', 'S Phone', 'S Box', 'S Tree No Leaves', 'S Primitive Sphere', 'S Table Bar Rectangle', 'Sk Cat 01', 'S Table Sit Rectangle', 'S Chair Bar', 'S Bed Twin', 'S Chair Sofa', 'Sk Snake 01', 'S Chair Box', 'S Cabinet Dresser 05', 'S Gui', 'S Table Sit Circle'}\n",
            "[MeshDataset] Generated 47 text_embeddings\n",
            "[MeshDataset] Generated codes for 423 entrys\n",
            "dict_keys(['vertices', 'faces', 'face_edges', 'text_embeds', 'codes'])\n"
          ]
        }
      ],
      "source": [
        "labels = set(item[\"texts\"] for item in dataset.data)\n",
        "print(labels)\n",
        "dataset.embed_texts(transformer)\n",
        "dataset.generate_codes(autoencoder)\n",
        "print(dataset.data[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFa_p1G-dRma"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXZ0qgV3dRma"
      },
      "source": [
        "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9hV_xUQdRma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0621e2a-c01a-4e10-cc20-9dce893ac089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/200: 100%|██████████| 211/211 [01:58<00:00,  1.79it/s, loss=8.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 average loss: 9.074482623999717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/200: 100%|██████████| 211/211 [01:58<00:00,  1.78it/s, loss=7.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 average loss: 7.88637790860723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/200: 100%|██████████| 211/211 [01:55<00:00,  1.82it/s, loss=6.87]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 average loss: 7.082348733151694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/200: 100%|██████████| 211/211 [01:57<00:00,  1.80it/s, loss=6.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 average loss: 6.486401998601254          avg loss speed: 1.5280010899849596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/200: 100%|██████████| 211/211 [01:57<00:00,  1.79it/s, loss=5.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 average loss: 6.0004782993081625          avg loss speed: 1.151231247478564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/200: 100%|██████████| 211/211 [02:01<00:00,  1.74it/s, loss=5.53]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 average loss: 5.620637932094917          avg loss speed: 0.9024384115921196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/200: 100%|██████████| 211/211 [02:00<00:00,  1.76it/s, loss=5.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 average loss: 5.3080258742327935          avg loss speed: 0.7278135357686502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/200: 100%|██████████| 211/211 [01:57<00:00,  1.80it/s, loss=5.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 average loss: 5.026195781490814          avg loss speed: 0.6168515870544766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/200: 100%|██████████| 211/211 [01:56<00:00,  1.81it/s, loss=4.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 average loss: 4.724442596119162          avg loss speed: 0.5938439331536802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/200:  70%|██████▉   | 147/211 [01:56<00:47,  1.35it/s, loss=3.25]"
          ]
        }
      ],
      "source": [
        "if not is_train_mesh_transformer:\n",
        "  trainer = MeshTransformerTrainer(model = transformer, warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=1)\n",
        "  trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "  transformer = trainer.model\n",
        "else:\n",
        "  trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
        "                                 learning_rate = 5e-4, batch_size=2)\n",
        "  loss = trainer.train(200, stop_at_loss = 0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the latest model**"
      ],
      "metadata": {
        "id": "TI5IM_Z3K26g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=8,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "autencoder = autoencoder_trainer.model\n",
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = True\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    # attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ").to(\"cuda\")\n",
        "\n",
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "transformer = trainer.model\n"
      ],
      "metadata": {
        "id": "Q8by6SXp4GHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIn8JVsNdRma"
      },
      "source": [
        "## Generate and view mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRlzLzYxdRma"
      },
      "outputs": [],
      "source": [
        "trainer.save(f'{working_dir}/mesh-transformer_{project_name}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX4RM2-ddRma"
      },
      "outputs": [],
      "source": [
        "def combind_mesh(path, mesh):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for r, faces_coordinates in enumerate(mesh):\n",
        "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "\n",
        "        for vertex in numpy_data:\n",
        "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "        for i in range(1, len(numpy_data), 3):\n",
        "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "        vertex_offset += len(numpy_data)\n",
        "\n",
        "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "def combind_mesh_with_rows(path, meshes):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for row, mesh in enumerate(meshes):\n",
        "        for r, faces_coordinates in enumerate(mesh):\n",
        "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)\n",
        "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)\n",
        "\n",
        "            for vertex in numpy_data:\n",
        "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "            for i in range(1, len(numpy_data), 3):\n",
        "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "            vertex_offset += len(numpy_data)\n",
        "\n",
        "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "\n",
        "def write_mesh_output(path, coords):\n",
        "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "    obj_file_content = \"\"\n",
        "\n",
        "    for vertex in numpy_data:\n",
        "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
        "\n",
        "    for i in range(1, len(numpy_data), 3):\n",
        "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
        "\n",
        "    with open(path, \"w\") as file:\n",
        "        file.write(obj_file_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdd-0bMJdRma"
      },
      "source": [
        "**Using only text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzAkhWM7dRmb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "text_coords = []\n",
        "for text in labels:\n",
        "    print(f\"Generating {text}\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0)\n",
        "    text_coords.append(faces_coordinates)\n",
        "\n",
        "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZP_cLvdRmb"
      },
      "source": [
        "**Text + prompt of tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8GVxRnrdRmb"
      },
      "source": [
        "Grab fresh copy of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ni22DzdRmb"
      },
      "outputs": [],
      "source": [
        "dataset = MeshDataset.load(dataset_path)\n",
        "dataset.generate_codes(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJxZBNUdRmb"
      },
      "source": [
        "**Prompt with 10% of codes/tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NTGFLFdRmb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "token_length_procent = 0.10\n",
        "codes = []\n",
        "texts = []\n",
        "for label in labels:\n",
        "    for item in dataset.data:\n",
        "        if item['texts'] == label:\n",
        "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "            texts.append(item['texts'])\n",
        "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "            break\n",
        "\n",
        "folder = working_dir / f'renders/text+codes'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "coords = []\n",
        "\n",
        "\n",
        "\n",
        "for text, prompt in zip(texts, codes):\n",
        "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "    coords.append(faces_coordinates)\n",
        "\n",
        "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "    write_mesh_output(obj_file_path, faces_coordinates)\n",
        "\n",
        "    print(obj_file_path)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
        "\n",
        "if text_coords is not None:\n",
        "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBZsZJtdRmb"
      },
      "source": [
        "**Prompt with 0% to 80% of tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUYkxcwdRmc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from pathlib import Path\n",
        "\n",
        "def convert_to_obj(vertices, faces, output_file_path):\n",
        "    scene = trimesh.Scene()\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "    scene.add_geometry(mesh)\n",
        "\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        f.write(scene.export(file_type=\"obj\"))\n",
        "\n",
        "def encode_to_pua(codes):\n",
        "    flat_codes = [item for sublist in codes for subsublist in sublist for item in subsublist]\n",
        "    return \"\".join(chr(code + 0xF0000) for code in flat_codes)\n",
        "\n",
        "jsonl_lines = []\n",
        "\n",
        "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
        "    codes = []\n",
        "    texts = []\n",
        "    for label in labels:\n",
        "        for item in dataset.data:\n",
        "            if item['texts'] == label:\n",
        "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "                texts.append(item['texts'])\n",
        "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "                break\n",
        "\n",
        "    coords = []\n",
        "    for text, code in zip(texts, codes):\n",
        "        print(f\"Generating {text} with {code.shape[1]} tokens\")\n",
        "        faces_coordinates = transformer.generate(texts=[text], prompt=code, temperature=0)\n",
        "        coords.append(faces_coordinates)\n",
        "\n",
        "        # Process mesh data inlined here\n",
        "        continuous_coors_list = [np_array.tolist() for np_array in faces_coordinates]\n",
        "        flat_list = [item for sublist in continuous_coors_list for item in sublist]\n",
        "        vertices = [vertex for sublist in flat_list for vertex in sublist]\n",
        "        faces = [[i, i + 1, i + 2] for i in range(0, len(vertices), 3)]\n",
        "\n",
        "        obj_filename = f'{text}_{code.shape[1]}_tokens.obj'\n",
        "        obj_file_path = folder / obj_filename\n",
        "        convert_to_obj(vertices, faces, obj_file_path)\n",
        "\n",
        "        encoded_codes = encode_to_pua(code.cpu().tolist())\n",
        "\n",
        "        with open(obj_file_path, \"r\") as file:\n",
        "            obj_contents = file.read()\n",
        "\n",
        "        # Append line to JSONL structure\n",
        "        jsonl_line = [\n",
        "            {\"role\": \"system\", \"content\": \"This assistant can understand 3D models using the meshgpt-pytorch Unicode plane 15 codebook for 16384 triangles and the .obj 3d format.\"},\n",
        "            {\"role\": \"user\", \"content\": encoded_codes},\n",
        "            {\"role\": \"assistant\", \"content\": obj_contents}\n",
        "        ]\n",
        "        jsonl_lines.append(jsonl_line)\n",
        "\n",
        "        print(obj_file_path)\n",
        "\n",
        "    mesh_rows.append(coords)\n",
        "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
        "\n",
        "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
        "\n",
        "with open(\"chatml.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in jsonl_lines:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}