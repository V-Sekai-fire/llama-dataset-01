{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V-Sekai-fire/meshgpt-dataset-01/blob/main/MeshGPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_train_autoencoder = False\n",
        "is_train_mesh_transformer = True\n",
        "is_train_autoencoder_iteration = True\n",
        "is_clear_dataset_npz = True"
      ],
      "metadata": {
        "id": "NM_rRocQAcZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aFoYTLhXhTlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tmLTKtUdRmQ"
      },
      "outputs": [],
      "source": [
        "!yes | pip uninstall typing-extensions\n",
        "!yes | pip install git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
        "!yes | pip install -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "!yes | pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")"
      ],
      "metadata": {
        "id": "5ztZ1JUl8zOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aW7oUHedRmQ"
      },
      "outputs": [],
      "source": [
        "import trimesh\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "max_faces = 5000\n",
        "\n",
        "def get_mesh(file_path):\n",
        "    mesh = trimesh.load(file_path, force='mesh')\n",
        "\n",
        "    # Center and scale vertices\n",
        "    center = np.mean(mesh.vertices, axis=0)\n",
        "    vertices = mesh.vertices - center\n",
        "    max_abs = np.max(np.abs(vertices))\n",
        "    scale_factor = (1 / 128) / max_abs\n",
        "    vertices *= scale_factor\n",
        "\n",
        "    # Quantize vertices\n",
        "    vertices = np.around(vertices).astype(np.float32)\n",
        "\n",
        "    # Sort vertices by Z, Y, X\n",
        "    sorted_indices = np.lexsort(vertices.T[::-1])\n",
        "    vertices = vertices[sorted_indices]\n",
        "\n",
        "    # Map old indices to new, sorted indices\n",
        "    vertex_map = np.empty(len(sorted_indices), dtype=int)\n",
        "    vertex_map[sorted_indices] = np.arange(len(sorted_indices))\n",
        "\n",
        "    # Reindex faces\n",
        "    reindexed_faces = vertex_map[mesh.faces]\n",
        "    sorted_faces = np.sort(reindexed_faces, axis=1)\n",
        "\n",
        "    return vertices, sorted_faces\n",
        "\n",
        "def augment_mesh(vertices, jitter_strength=0.01):\n",
        "    jitter_amount = np.random.uniform(-jitter_strength, jitter_strength, size=vertices.shape)\n",
        "    vertices += jitter_amount\n",
        "    return vertices\n",
        "\n",
        "def snake_to_sentence_case(snake_str):\n",
        "    components = snake_str.split(\"_\")\n",
        "    return \" \".join(word.capitalize() for word in components)\n",
        "\n",
        "def load_filename(directory, variations):\n",
        "    obj_datas = []\n",
        "\n",
        "    # Get random scale factors within a range\n",
        "    scale_factors = np.random.uniform(0.75, 1.0, size=variations)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".glb\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vertices, faces = get_mesh(file_path)\n",
        "\n",
        "            if len(faces) > max_faces:\n",
        "                print(f\"Mesh {filename} has {len(faces)} faces which is more than the allowed {max_faces} faces. Rejecting.\")\n",
        "                continue\n",
        "\n",
        "            faces_tensor = torch.tensor(faces, dtype=torch.long).to(\"cuda\")\n",
        "            face_edges = derive_face_edges_from_faces(faces_tensor)\n",
        "\n",
        "            texts, _ = os.path.splitext(filename)\n",
        "            texts = snake_to_sentence_case(texts)\n",
        "            for scale_factor in scale_factors:\n",
        "                aug_vertices = augment_mesh(vertices.copy()) * scale_factor\n",
        "                aug_vertices_tensor = torch.tensor(aug_vertices, dtype=torch.float)\n",
        "\n",
        "                obj_data = {\n",
        "                    \"vertices\": aug_vertices_tensor.to(\"cuda\"),\n",
        "                    \"faces\": faces_tensor,\n",
        "                    \"face_edges\": face_edges,\n",
        "                    \"texts\": texts\n",
        "                }\n",
        "                obj_datas.append(obj_data)\n",
        "\n",
        "    print(f\"[create_mesh_dataset] Returning {len(obj_datas)} meshes\")\n",
        "    return obj_datas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vthmcnU2dRmS"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from meshgpt_pytorch import MeshDataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "project_name = \"demo_mesh\"\n",
        "\n",
        "working_dir = f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "working_dir = Path(working_dir)\n",
        "working_dir.mkdir(exist_ok = True, parents = True)\n",
        "dataset_path = working_dir / (project_name + \".npz\")\n",
        "\n",
        "\n",
        "if is_clear_dataset_npz:\n",
        "    data = load_filename(working_dir, 2)\n",
        "    dataset = MeshDataset(data)\n",
        "    dataset.generate_face_edges()\n",
        "    dataset.save(dataset_path)\n",
        "else:\n",
        "    dataset = MeshDataset.load(dataset_path)\n",
        "\n",
        "print(dataset.data[0].keys())\n",
        "print(set(item[\"texts\"] for item in dataset.data)  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVsRDtzdRmT"
      },
      "source": [
        "### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPpalWsbdRmU"
      },
      "outputs": [],
      "source": [
        "seen_texts = []  # Keep track of seen texts to avoid processing duplicates\n",
        "mesh_list = []  # List to store individual meshes\n",
        "translation_distance = 1.0  # Distance to translate vertices, set to 1 unit as required\n",
        "\n",
        "# Iterate over each item in the dataset\n",
        "for r, item in enumerate(dataset.data):\n",
        "    texts = item['texts']\n",
        "\n",
        "    # Skip this iteration if there are no texts or if we've already seen these texts\n",
        "    if len(texts) == 0 or texts in seen_texts:\n",
        "        continue\n",
        "\n",
        "    # Add these texts to the list of seen texts\n",
        "    seen_texts.append(texts)\n",
        "\n",
        "    # Get vertices and faces\n",
        "    vertices = np.array(item['vertices'].cpu())\n",
        "    faces = np.array(item['faces'].cpu())\n",
        "\n",
        "    # Translate the vertices copy\n",
        "    translation_vector = np.array([len(seen_texts) * translation_distance, 0, 0])  # Translation along the x-axis\n",
        "    vertices[:, :3] += translation_vector  # Apply translation only to x, y, z\n",
        "\n",
        "    # Create a new mesh object with the translated vertices and original faces\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "\n",
        "    # Append the new mesh to our list of meshes\n",
        "    mesh_list.append(mesh)\n",
        "\n",
        "    print(f\"Iteration {r} complete. Processed {len(vertices)} vertices and {len(faces)} faces.\")\n",
        "\n",
        "# After iterating over all items, print the number of processed meshes\n",
        "print(f\"Total number of processed meshes: {len(mesh_list)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pHZ20udRmU"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoAxJWecdRmV"
      },
      "outputs": [],
      "source": [
        "autoencoder = MeshAutoencoder().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJewvY1dRmV"
      },
      "source": [
        "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ9qMRVxdRmW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "seed_value = 42\n",
        "random.seed(seed_value)\n",
        "\n",
        "random.shuffle(dataset.data)\n",
        "\n",
        "initial_length = len(dataset.data)\n",
        "\n",
        "if initial_length > 0:\n",
        "    replication_factor = max(1, 400 // initial_length)\n",
        "    dataset.data *= replication_factor\n",
        "    while len(dataset.data) <= 2000 - initial_length:\n",
        "        dataset.data += [dict(d) for d in dataset.data]\n",
        "    if len(dataset.data) > 2000:\n",
        "        dataset.data = dataset.data[:2000]\n",
        "\n",
        "print(len(dataset.data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9k-OAoBdRmY"
      },
      "source": [
        "**Train to about 0.3 loss if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKTuiTHvdRmY"
      },
      "outputs": [],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
        "                                            batch_size=8,\n",
        "                                            grad_accum_every=2,\n",
        "                                            learning_rate = 4e-3)\n",
        "if is_train_autoencoder:\n",
        "  if is_train_autoencoder_iteration:\n",
        "    autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  loss = autoencoder_trainer.train(380,stop_at_loss = 0.28, diplay_graph= True)\n",
        "  autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "else:\n",
        "  autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  autencoder = autoencoder_trainer.model\n",
        "  for param in autoencoder.parameters():\n",
        "      param.requires_grad = True\n",
        "  import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLCezPnNdRmZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Decoder total parameters: {total_params}\")\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8sv-udRmZ"
      },
      "source": [
        "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
        "\n",
        "**If you don't;** <br>\n",
        "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
        "<br>\n",
        "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
        "\n",
        "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s--ya8W0dRmZ"
      },
      "outputs": [],
      "source": [
        "labels = set(item[\"texts\"] for item in dataset.data)\n",
        "print(labels)\n",
        "dataset.embed_texts(transformer)\n",
        "dataset.generate_codes(autoencoder)\n",
        "print(dataset.data[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFa_p1G-dRma"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXZ0qgV3dRma"
      },
      "source": [
        "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9hV_xUQdRma"
      },
      "outputs": [],
      "source": [
        "if not is_train_mesh_transformer:\n",
        "  trainer = MeshTransformerTrainer(model = transformer, warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=1)\n",
        "  trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "  transformer = trainer.model\n",
        "else:\n",
        "  trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
        "                                 learning_rate = 5e-4, batch_size=2)\n",
        "  loss = trainer.train(200, stop_at_loss = 0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the latest model**"
      ],
      "metadata": {
        "id": "TI5IM_Z3K26g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=8,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "autencoder = autoencoder_trainer.model\n",
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = True\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    # attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ").to(\"cuda\")\n",
        "\n",
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "transformer = trainer.model\n"
      ],
      "metadata": {
        "id": "Q8by6SXp4GHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIn8JVsNdRma"
      },
      "source": [
        "## Generate and view mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRlzLzYxdRma"
      },
      "outputs": [],
      "source": [
        "trainer.save(f'{working_dir}/mesh-transformer_{project_name}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX4RM2-ddRma"
      },
      "outputs": [],
      "source": [
        "def combind_mesh(path, mesh):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for r, faces_coordinates in enumerate(mesh):\n",
        "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "\n",
        "        for vertex in numpy_data:\n",
        "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "        for i in range(1, len(numpy_data), 3):\n",
        "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "        vertex_offset += len(numpy_data)\n",
        "\n",
        "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "def combind_mesh_with_rows(path, meshes):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for row, mesh in enumerate(meshes):\n",
        "        for r, faces_coordinates in enumerate(mesh):\n",
        "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)\n",
        "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)\n",
        "\n",
        "            for vertex in numpy_data:\n",
        "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "            for i in range(1, len(numpy_data), 3):\n",
        "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "            vertex_offset += len(numpy_data)\n",
        "\n",
        "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "\n",
        "def write_mesh_output(path, coords):\n",
        "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "    obj_file_content = \"\"\n",
        "\n",
        "    for vertex in numpy_data:\n",
        "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
        "\n",
        "    for i in range(1, len(numpy_data), 3):\n",
        "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
        "\n",
        "    with open(path, \"w\") as file:\n",
        "        file.write(obj_file_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdd-0bMJdRma"
      },
      "source": [
        "**Using only text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzAkhWM7dRmb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "text_coords = []\n",
        "for text in labels:\n",
        "    print(f\"Generating {text}\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0)\n",
        "    text_coords.append(faces_coordinates)\n",
        "\n",
        "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZP_cLvdRmb"
      },
      "source": [
        "**Text + prompt of tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8GVxRnrdRmb"
      },
      "source": [
        "Grab fresh copy of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ni22DzdRmb"
      },
      "outputs": [],
      "source": [
        "dataset = MeshDataset.load(dataset_path)\n",
        "dataset.generate_codes(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJxZBNUdRmb"
      },
      "source": [
        "**Prompt with 10% of codes/tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NTGFLFdRmb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "token_length_procent = 0.10\n",
        "codes = []\n",
        "texts = []\n",
        "for label in labels:\n",
        "    for item in dataset.data:\n",
        "        if item['texts'] == label:\n",
        "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "            texts.append(item['texts'])\n",
        "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "            break\n",
        "\n",
        "folder = working_dir / f'renders/text+codes'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "coords = []\n",
        "\n",
        "\n",
        "\n",
        "for text, prompt in zip(texts, codes):\n",
        "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "    coords.append(faces_coordinates)\n",
        "\n",
        "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "    write_mesh_output(obj_file_path, faces_coordinates)\n",
        "\n",
        "    print(obj_file_path)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
        "\n",
        "if text_coords is not None:\n",
        "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBZsZJtdRmb"
      },
      "source": [
        "**Prompt with 0% to 80% of tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUYkxcwdRmc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from pathlib import Path\n",
        "\n",
        "def convert_to_obj(vertices, faces, output_file_path):\n",
        "    scene = trimesh.Scene()\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "    scene.add_geometry(mesh)\n",
        "\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        f.write(scene.export(file_type=\"obj\"))\n",
        "\n",
        "def encode_to_pua(codes):\n",
        "    flat_codes = [item for sublist in codes for subsublist in sublist for item in subsublist]\n",
        "    return \"\".join(chr(code + 0xF0000) for code in flat_codes)\n",
        "\n",
        "jsonl_lines = []\n",
        "\n",
        "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
        "    codes = []\n",
        "    texts = []\n",
        "    for label in labels:\n",
        "        for item in dataset.data:\n",
        "            if item['texts'] == label:\n",
        "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "                texts.append(item['texts'])\n",
        "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "                break\n",
        "\n",
        "    coords = []\n",
        "    for text, code in zip(texts, codes):\n",
        "        print(f\"Generating {text} with {code.shape[1]} tokens\")\n",
        "        faces_coordinates = transformer.generate(texts=[text], prompt=code, temperature=0)\n",
        "        coords.append(faces_coordinates)\n",
        "\n",
        "        # Process mesh data inlined here\n",
        "        continuous_coors_list = [np_array.tolist() for np_array in faces_coordinates]\n",
        "        flat_list = [item for sublist in continuous_coors_list for item in sublist]\n",
        "        vertices = [vertex for sublist in flat_list for vertex in sublist]\n",
        "        faces = [[i, i + 1, i + 2] for i in range(0, len(vertices), 3)]\n",
        "\n",
        "        obj_filename = f'{text}_{code.shape[1]}_tokens.obj'\n",
        "        obj_file_path = folder / obj_filename\n",
        "        convert_to_obj(vertices, faces, obj_file_path)\n",
        "\n",
        "        encoded_codes = encode_to_pua(code.cpu().tolist())\n",
        "\n",
        "        with open(obj_file_path, \"r\") as file:\n",
        "            obj_contents = file.read()\n",
        "\n",
        "        # Append line to JSONL structure\n",
        "        jsonl_line = [\n",
        "            {\"role\": \"system\", \"content\": \"This assistant can understand 3D models using the meshgpt-pytorch Unicode plane 15 codebook for 16384 triangles and the .obj 3d format.\"},\n",
        "            {\"role\": \"user\", \"content\": encoded_codes},\n",
        "            {\"role\": \"assistant\", \"content\": obj_contents}\n",
        "        ]\n",
        "        jsonl_lines.append(jsonl_line)\n",
        "\n",
        "        print(obj_file_path)\n",
        "\n",
        "    mesh_rows.append(coords)\n",
        "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
        "\n",
        "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
        "\n",
        "with open(\"chatml.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in jsonl_lines:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}