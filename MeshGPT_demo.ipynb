{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aFoYTLhXhTlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tmLTKtUdRmQ"
      },
      "outputs": [],
      "source": [
        "!yes | pip install git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
        "!yes | pip install -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "!yes | pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aW7oUHedRmQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")\n",
        "\n",
        "def get_mesh(file_path):\n",
        "    mesh = trimesh.load(file_path, force='mesh')\n",
        "\n",
        "    vertices = mesh.vertices.tolist()\n",
        "    faces = mesh.faces.tolist()\n",
        "    # Center\n",
        "    centered_vertices = vertices - np.mean(vertices, axis=0)\n",
        "\n",
        "    # Calculate the padding and scale factor numerically\n",
        "    padding_fraction = 1 / 128                    # 0.0078125\n",
        "    total_padding = padding_fraction * 2          # 0.015625\n",
        "    scale_factor = 1 - total_padding              # 0.984375\n",
        "\n",
        "    max_abs = np.max(np.abs(centered_vertices))\n",
        "    vertices = centered_vertices / (max_abs / scale_factor)\n",
        "\n",
        "    # Sort by Z, Y,X where Z is vertical\n",
        "    def sort_vertices(vertex):\n",
        "        return vertex[2], vertex[1], vertex[0]\n",
        "\n",
        "    seen = OrderedDict()\n",
        "    for point in vertices:\n",
        "      key = tuple(point)\n",
        "      if key not in seen:\n",
        "        seen[key] = point\n",
        "\n",
        "    unique_vertices =  list(seen.values())\n",
        "    sorted_vertices = sorted(unique_vertices, key=sort_vertices)\n",
        "\n",
        "    vertices_as_tuples = [tuple(v) for v in vertices]\n",
        "    sorted_vertices_as_tuples = [tuple(v) for v in sorted_vertices]\n",
        "\n",
        "    vertex_map = {old_index: new_index for old_index, vertex_tuple in enumerate(vertices_as_tuples) for new_index, sorted_vertex_tuple in enumerate(sorted_vertices_as_tuples) if vertex_tuple == sorted_vertex_tuple}\n",
        "    reindexed_faces = [[vertex_map[face[0]], vertex_map[face[1]], vertex_map[face[2]]] for face in faces]\n",
        "    sorted_faces = [sorted(sub_arr) for sub_arr in reindexed_faces]\n",
        "\n",
        "    return np.array(sorted_vertices), np.array(sorted_faces)\n",
        "\n",
        "\n",
        "def augment_mesh(vertices, scale_factor):\n",
        "    jitter_factor=0.01\n",
        "    possible_values = np.arange(-jitter_factor, jitter_factor , 0.0005)\n",
        "    offsets = np.random.choice(possible_values, size=vertices.shape)\n",
        "    vertices = vertices + offsets\n",
        "\n",
        "    vertices = vertices * scale_factor\n",
        "    return vertices\n",
        "\n",
        "\n",
        "def load_filename(directory, variations):\n",
        "    obj_datas = []\n",
        "    chosen_models_count = {}\n",
        "    possible_values = np.arange(0.75, 1.0 , 0.005)\n",
        "    scale_factors = np.random.choice(possible_values, size=variations)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith((\".glb\")):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vertices, faces = get_mesh(file_path)\n",
        "\n",
        "            faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"cuda\")\n",
        "            face_edges =  derive_face_edges_from_faces(faces)\n",
        "            texts, ext = os.path.splitext(filename)\n",
        "\n",
        "            for scale_factor in scale_factors:\n",
        "                aug_vertices = augment_mesh(vertices.copy(), scale_factor)\n",
        "                obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"cuda\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": texts }\n",
        "                obj_datas.append(obj_data)\n",
        "\n",
        "    print(f\"[create_mesh_dataset] Returning {len(obj_data)} meshes\")\n",
        "    return obj_datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vthmcnU2dRmS"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from meshgpt_pytorch import MeshDataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "project_name = \"demo_mesh\"\n",
        "\n",
        "working_dir = f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "working_dir = Path(working_dir)\n",
        "working_dir.mkdir(exist_ok = True, parents = True)\n",
        "dataset_path = working_dir / (project_name + \".npz\")\n",
        "\n",
        "\n",
        "if True or not os.path.isfile(dataset_path):\n",
        "    data = load_filename(working_dir, 20)\n",
        "    dataset = MeshDataset(data)\n",
        "    dataset.generate_face_edges()\n",
        "    print(set(item[\"texts\"] for item in dataset.data)  )\n",
        "    dataset.save(dataset_path)\n",
        "else:\n",
        "    dataset = MeshDataset.load(dataset_path)\n",
        "    print(dataset.data[0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVsRDtzdRmT"
      },
      "source": [
        "### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPpalWsbdRmU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "folder = f\"{working_dir}/renders\"\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "all_vertices = []\n",
        "all_faces = []\n",
        "vertex_offset = 0\n",
        "translation_distance = 0.5\n",
        "\n",
        "for r, item in enumerate(dataset.data):\n",
        "    vertices_copy =  np.copy(item['vertices'].cpu())\n",
        "    vertices_copy += translation_distance * (r / 0.2 - 1)\n",
        "\n",
        "    for vert in vertices_copy:\n",
        "        vertex = vert\n",
        "        all_vertices.append(f\"v {float(vertex[0])}  {float(vertex[1])}  {float(vertex[2])}\\n\")\n",
        "    for face in item['faces']:\n",
        "        all_faces.append(f\"f {face[0]+1+ vertex_offset} {face[1]+ 1+vertex_offset} {face[2]+ 1+vertex_offset}\\n\")\n",
        "    vertex_offset = len(all_vertices)\n",
        "\n",
        "obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "obj_file_path = f'{folder}/3d_models_inspect.obj'\n",
        "\n",
        "with open(obj_file_path, \"w\") as file:\n",
        "    file.write(obj_file_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pHZ20udRmU"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoAxJWecdRmV"
      },
      "outputs": [],
      "source": [
        "autoencoder = MeshAutoencoder().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJewvY1dRmV"
      },
      "source": [
        "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ9qMRVxdRmW"
      },
      "outputs": [],
      "source": [
        "dataset.data = [dict(d) for d in dataset.data] * 1\n",
        "print(len(dataset.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgFGxnXAdRmX"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NWrpCCgdRmX"
      },
      "outputs": [],
      "source": [
        "# autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=8,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "# autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "# autencoder = autoencoder_trainer.model\n",
        "# for param in autoencoder.parameters():\n",
        "#     param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9k-OAoBdRmY"
      },
      "source": [
        "**Train to about 0.3 loss if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKTuiTHvdRmY"
      },
      "outputs": [],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
        "                                             batch_size=8,\n",
        "                                             grad_accum_every=2,\n",
        "                                             learning_rate = 4e-3)\n",
        "loss = autoencoder_trainer.train(280,stop_at_loss = 0.28, diplay_graph= True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')"
      ],
      "metadata": {
        "id": "Ez6k8iTJrdY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLCezPnNdRmZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Decoder total parameters: {total_params}\")\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8sv-udRmZ"
      },
      "source": [
        "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
        "\n",
        "**If you don't;** <br>\n",
        "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
        "<br>\n",
        "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
        "\n",
        "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s--ya8W0dRmZ"
      },
      "outputs": [],
      "source": [
        "labels = set(item[\"texts\"] for item in dataset.data)\n",
        "print(labels)\n",
        "dataset.embed_texts(transformer)\n",
        "dataset.generate_codes(autoencoder)\n",
        "print(dataset.data[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFa_p1G-dRma"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9hV_xUQdRma"
      },
      "outputs": [],
      "source": [
        "# trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "# trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "# transformer = trainer.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXZ0qgV3dRma"
      },
      "source": [
        "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhcQE6XIdRma"
      },
      "outputs": [],
      "source": [
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
        "                                 learning_rate = 5e-4, batch_size=2)\n",
        "loss = trainer.train(200, stop_at_loss = 0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRlzLzYxdRma"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer.save(f'{working_dir}/mesh-transformer_{project_name}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIn8JVsNdRma"
      },
      "source": [
        "## Generate and view mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX4RM2-ddRma"
      },
      "outputs": [],
      "source": [
        "def combind_mesh(path, mesh):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for r, faces_coordinates in enumerate(mesh):\n",
        "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "\n",
        "        for vertex in numpy_data:\n",
        "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "        for i in range(1, len(numpy_data), 3):\n",
        "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "        vertex_offset += len(numpy_data)\n",
        "\n",
        "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "def combind_mesh_with_rows(path, meshes):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for row, mesh in enumerate(meshes):\n",
        "        for r, faces_coordinates in enumerate(mesh):\n",
        "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)\n",
        "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)\n",
        "\n",
        "            for vertex in numpy_data:\n",
        "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "            for i in range(1, len(numpy_data), 3):\n",
        "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "            vertex_offset += len(numpy_data)\n",
        "\n",
        "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "\n",
        "def write_mesh_output(path, coords):\n",
        "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "    obj_file_content = \"\"\n",
        "\n",
        "    for vertex in numpy_data:\n",
        "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
        "\n",
        "    for i in range(1, len(numpy_data), 3):\n",
        "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
        "\n",
        "    with open(path, \"w\") as file:\n",
        "        file.write(obj_file_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdd-0bMJdRma"
      },
      "source": [
        "**Using only text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzAkhWM7dRmb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "text_coords = []\n",
        "for text in labels:\n",
        "    print(f\"Generating {text}\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0)\n",
        "    text_coords.append(faces_coordinates)\n",
        "\n",
        "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZP_cLvdRmb"
      },
      "source": [
        "**Text + prompt of tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8GVxRnrdRmb"
      },
      "source": [
        "Grab fresh copy of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ni22DzdRmb"
      },
      "outputs": [],
      "source": [
        "dataset = MeshDataset.load(dataset_path)\n",
        "dataset.generate_codes(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJxZBNUdRmb"
      },
      "source": [
        "**Prompt with 10% of codes/tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NTGFLFdRmb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "token_length_procent = 0.10\n",
        "codes = []\n",
        "texts = []\n",
        "for label in labels:\n",
        "    for item in dataset.data:\n",
        "        if item['texts'] == label:\n",
        "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "            texts.append(item['texts'])\n",
        "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "            break\n",
        "\n",
        "folder = working_dir / f'renders/text+codes'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "coords = []\n",
        "\n",
        "\n",
        "\n",
        "for text, prompt in zip(texts, codes):\n",
        "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "    coords.append(faces_coordinates)\n",
        "\n",
        "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "    write_mesh_output(obj_file_path, faces_coordinates)\n",
        "\n",
        "    print(obj_file_path)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
        "\n",
        "if text_coords is not None:\n",
        "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBZsZJtdRmb"
      },
      "source": [
        "**Prompt with 0% to 80% of tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUYkxcwdRmc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from pathlib import Path\n",
        "\n",
        "def convert_to_obj(vertices, faces, output_file_path):\n",
        "    scene = trimesh.Scene()\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "    scene.add_geometry(mesh)\n",
        "\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        f.write(scene.export(file_type=\"obj\"))\n",
        "\n",
        "def encode_to_pua(codes):\n",
        "    flat_codes = [item for sublist in codes for subsublist in sublist for item in subsublist]\n",
        "    return \"\".join(chr(code + 0xF0000) for code in flat_codes)\n",
        "\n",
        "jsonl_lines = []\n",
        "\n",
        "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
        "    codes = []\n",
        "    texts = []\n",
        "    for label in labels:\n",
        "        for item in dataset.data:\n",
        "            if item['texts'] == label:\n",
        "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "                texts.append(item['texts'])\n",
        "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "                break\n",
        "\n",
        "    coords = []\n",
        "    for text, code in zip(texts, codes):\n",
        "        print(f\"Generating {text} with {code.shape[1]} tokens\")\n",
        "        faces_coordinates = transformer.generate(texts=[text], prompt=code, temperature=0)\n",
        "        coords.append(faces_coordinates)\n",
        "\n",
        "        # Process mesh data inlined here\n",
        "        continuous_coors_list = [np_array.tolist() for np_array in faces_coordinates]\n",
        "        flat_list = [item for sublist in continuous_coors_list for item in sublist]\n",
        "        vertices = [vertex for sublist in flat_list for vertex in sublist]\n",
        "        faces = [[i, i + 1, i + 2] for i in range(0, len(vertices), 3)]\n",
        "\n",
        "        obj_filename = f'{text}_{code.shape[1]}_tokens.obj'\n",
        "        obj_file_path = folder / obj_filename\n",
        "        convert_to_obj(vertices, faces, obj_file_path)\n",
        "\n",
        "        encoded_codes = encode_to_pua(code.cpu().tolist())\n",
        "\n",
        "        with open(obj_file_path, \"r\") as file:\n",
        "            obj_contents = file.read()\n",
        "\n",
        "        # Append line to JSONL structure\n",
        "        jsonl_line = [\n",
        "            {\"role\": \"system\", \"content\": \"This assistant can understand 3D models using the meshgpt-pytorch Unicode plane 15 codebook for 16384 triangles and the .obj 3d format.\"},\n",
        "            {\"role\": \"user\", \"content\": encoded_codes},\n",
        "            {\"role\": \"assistant\", \"content\": obj_contents}\n",
        "        ]\n",
        "        jsonl_lines.append(jsonl_line)\n",
        "\n",
        "        print(obj_file_path)\n",
        "\n",
        "    mesh_rows.append(coords)\n",
        "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
        "\n",
        "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
        "\n",
        "with open(\"chatml.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in jsonl_lines:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=8,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "# autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "# autencoder = autoencoder_trainer.model\n",
        "# for param in autoencoder.parameters():\n",
        "#     param.requires_grad = True\n",
        "# trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "# trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "# transformer = trainer.model\n"
      ],
      "metadata": {
        "id": "Q8by6SXp4GHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}